{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('word_to_id.pickle', 'rb') as handle:\n",
    "    word_to_id = pickle.load(handle)\n",
    "\n",
    "with open('id_to_word.pickle', 'rb') as handle:\n",
    "    id_to_word = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    vocab_size = 10000\n",
    "    batch_size = 20\n",
    "    num_steps = 20  # sequence length; the number of unrolls\n",
    "    hidden_size = 200  # number of hidden units in LSTM; also embedding size\n",
    "    keep_prob = 0.5  # 1 - dropoff rate\n",
    "    num_layers = 2  # number of LSTM layers\n",
    "    max_grad_norm = 5  # max gradient (to prevent the exploding gradient problems)\n",
    "    init_scale = 0.1  # the initial scale of the weights\n",
    "    max_epoch = 4  # the number of epochs trained with the initial learning rate\n",
    "    max_max_epoch = 13  # the total number of epochs for training\n",
    "    learning_rate = 1.0  # the initial value of the learning rate\n",
    "    lr_decay = 0.5  # the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "\n",
    "eval_config = config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    \"\"\"The PTB model.\"\"\"\n",
    "\n",
    "    def __init__(self, is_training, config, input_=None):\n",
    "        batch_size = config.batch_size\n",
    "        num_steps = config.num_steps\n",
    "        hidden_size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        if input_ is not None:\n",
    "            # For normal training and validation\n",
    "            self._input = input_\n",
    "            self._input_data = input_.input_data\n",
    "            self._targets = input_.targets\n",
    "            \n",
    "        else:\n",
    "            # For text generations\n",
    "            self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "            self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        def lstm_cell():\n",
    "            # With the latest TensorFlow source code (as of Mar 27, 2017),\n",
    "            # the BasicLSTMCell will need a reuse parameter which is unfortunately not\n",
    "            # defined in TensorFlow 1.0. To maintain backwards compatibility, we add\n",
    "            # an argument check here:\n",
    "            \n",
    "            if 'reuse' in inspect.getargspec(\n",
    "                    tf.contrib.rnn.BasicLSTMCell.__init__).args:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True,\n",
    "                    reuse=tf.get_variable_scope().reuse)\n",
    "            else:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True)\n",
    "            \n",
    "            # Note because we set `state_is_tuple=True`, the states are 2-tuples of the `c_state` and `h_state`\n",
    "            # `c_state` is the cell state\n",
    "            # `h_state` is the hidden state\n",
    "            # See this SO thread: https://stackoverflow.com/questions/41789133/c-state-and-m-state-in-tensorflow-lstm\n",
    "    \n",
    "        attn_cell = lstm_cell\n",
    "\n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "\n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(\n",
    "                    lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "\n",
    "        # Stacking multiple LSTMs\n",
    "        attn_cells = [attn_cell() for _ in range(config.num_layers)]\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell(attn_cells, state_is_tuple=True)\n",
    "        \n",
    "        # Initialize states with zeros\n",
    "        # `_initial_state` is a list of `num_layers` tensors\n",
    "        # Each is a tuple of (`c_state`, `h_state`),\n",
    "        # and both `c_state` and `h_state` are shaped [batch_size, hidden_size]\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # The word IDs will be embedded into a dense representation before feeding to the LSTM.\n",
    "        # This allows the model to efficiently represent the knowledge about particular words.\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, hidden_size], dtype=tf.float32)\n",
    "            input_embeddings = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "            # The shape of `input_embeddings` is [batch_size, num_steps, hidden_size]\n",
    "        \n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            input_embeddings = tf.nn.dropout(input_embeddings, config.keep_prob)\n",
    "\n",
    "        # Simplified version of models/tutorials/rnn/rnn.py's rnn().\n",
    "        # This builds an unrolled LSTM for tutorial purposes only.\n",
    "        # In general, use the rnn() or state_saving_rnn() from rnn.py.\n",
    "        #\n",
    "        # The alternative version of the code below is:\n",
    "        #\n",
    "        # inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "        # outputs, state = tf.contrib.rnn.static_rnn(\n",
    "        #     cell, inputs, initial_state=self._initial_state)\n",
    "        \n",
    "        # Unroll LSTM loop\n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        \n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                (cell_output, state) = stacked_lstm(input_embeddings[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "                # `outputs` is a list of `num_steps` tensors, each shaped [batch_size, hidden_size]\n",
    "        \n",
    "        # Resize the ouput into a [batch_size * num_steps, hidden_size] matrix.\n",
    "        # Note axis=1 because we want to group words together according to its original sequence\n",
    "        # in order to compare with `targets` to compute loss later.\n",
    "        output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, hidden_size])\n",
    "        \n",
    "        # Compute logits\n",
    "        softmax_w = tf.get_variable(\n",
    "            \"softmax_w\", [hidden_size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\n",
    "            \"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "        \n",
    "        self._logits = logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        # The shape of `logits` =\n",
    "        # [batch_size * num_steps, hidden_size] x [hidden_size, vocab_size] + [vocab_size] =\n",
    "        # [batch_size * num_steps, vocab_size]\n",
    "        \n",
    "        # Sample based on the size of logits (used for text generation)\n",
    "        self._logits_sample = tf.multinomial(logits, 1)\n",
    "        \n",
    "        # Reshape logits to be 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        # Source code: https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,  # shape: [batch_size, num_steps, vocab_size]\n",
    "            self._targets,  # shape: [batch_size, num_steps]\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "\n",
    "        # Update the cost variables\n",
    "        self._cost = cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # Optimizer\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "        \n",
    "        \n",
    "    # To update learning rate\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "    \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    \n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def logits_sample(self):\n",
    "        return self._logits_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use \"it\" as the start of the sentence\n",
    "feed = np.array(word_to_id['it']).reshape(1, 1)\n",
    "\n",
    "# Define sentence length\n",
    "text_length = 200\n",
    "\n",
    "def generate_text(session, model, feed, text_length):\n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"final_state\": model.final_state,\n",
    "        \"logits\": model.logits_sample\n",
    "    }\n",
    "    \n",
    "    generated_text = [feed]\n",
    "    \n",
    "    for i in range(text_length):\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.input_data] = feed\n",
    "        \n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        \n",
    "        # Extract final_state and sampled logits after the current step,\n",
    "        # which become the new state and feed for the next step\n",
    "        state = vals[\"final_state\"]\n",
    "        feed = vals[\"logits\"]\n",
    "        \n",
    "        # Append generated text\n",
    "        generated_text.append(feed)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'model_output_2017-07-25-19-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from file: model_output_2017-07-25-19-15/model.ckpt-29975\n",
      "('Sample text generation:', u\"it 's won in material and capability and large marketing institutional investors <eos> for capital rooms for instance that and the first decline at the major period in july of the october N rate account was low from the same year <eos> but but some analysts say that which pencil shows completed leaving societe louis 's turnaround <eos> cash exploration are insured around N last year <eos> <unk> employed by N issues <unk> asian up N <eos> commission <unk> funds <eos> we 're wind for responsibility at least this year when britain tax-loss <unk> all shrinking results in singapore <eos> no grab could still broaden inflation for the debt bill later they say is a lot of money and not use there is a hurry to happen ahead and on others <eos> in addition to which mr. davis N years old widened a ban for clothing as by <unk> author last week as much as a tokyo-based and voice of operations <eos> i would have some <unk> full-time issues to live the team 's ivan <unk> up morally the advocates ' promise <eos> in the N wages and the company 's office dunn had been a significant delicate <unk> <eos> but\")\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    \n",
    "    # Define model for text generations\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    with tf.name_scope(\"Feed\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            mfeed = PTBModel(is_training=False, config=eval_config)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        # Initialize variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        session.run(init)\n",
    "\n",
    "        # Restore model weights from previously saved model\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored from file: %s\" % ckpt.model_checkpoint_path)\n",
    "        \n",
    "        generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "        generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "        print(\"Sample text generation:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (General DS)",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
